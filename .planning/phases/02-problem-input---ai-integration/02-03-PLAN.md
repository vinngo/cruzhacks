---
phase: 02-problem-input---ai-integration
plan: 03
type: execute
wave: 2
depends_on: ["02-02"]
files_modified:
  - components/workspace/CanvasPanel.tsx
  - components/TldrawEditor.tsx
  - components/workspace/ChatPanel.tsx
  - app/api/chat/route.ts
autonomous: true

must_haves:
  truths:
    - "Canvas automatically captures screenshot when user pauses drawing"
    - "Screenshot capture is debounced (2-3 seconds after last change)"
    - "AI receives canvas screenshot along with problem and chat history"
    - "AI analyzes canvas work and asks questions about it"
    - "User sees 'AI is analyzing...' during screenshot processing"
  artifacts:
    - path: "components/workspace/CanvasPanel.tsx"
      provides: "Screenshot capture hook and debounce logic"
      min_lines: 80
      contains: "exportAs.*png"
    - path: "components/TldrawEditor.tsx"
      provides: "tldraw editor instance access via ref"
      min_lines: 40
      contains: "useEditor"
    - path: "app/api/chat/route.ts"
      provides: "Vision-enabled AI analysis with canvas screenshots"
      contains: "image"
  key_links:
    - from: "components/workspace/CanvasPanel.tsx"
      to: "components/TldrawEditor.tsx"
      via: "useRef to access editor.exportAs('png')"
      pattern: "exportAs"
    - from: "components/workspace/CanvasPanel.tsx"
      to: "components/workspace/ChatPanel.tsx"
      via: "context to trigger AI analysis"
      pattern: "canvasDataUrl|setCanvasDataUrl"
    - from: "app/api/chat/route.ts"
      to: "AI vision model"
      via: "messages array with image content"
      pattern: "type.*image"
---

<objective>
Implement automatic canvas screenshot capture with AI vision analysis integration.

Purpose: Enable AI to "see" student's canvas work by capturing screenshots when they pause, sending images to AI for analysis, and receiving Socratic questions about their work. Completes Phase 2's core value: AI analyzes canvas work against original problem.

Output: Working canvas-to-AI pipeline where pausing drawing triggers screenshot capture, AI analysis, and contextual guidance questions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-problem-input---ai-integration/02-CONTEXT.md
@.planning/phases/02-problem-input---ai-integration/02-02-SUMMARY.md

# Existing files
@components/workspace/CanvasPanel.tsx
@components/TldrawEditor.tsx
@app/api/chat/route.ts
</context>

<tasks>

<task type="auto">
  <name>Add screenshot capture capability to canvas</name>
  <files>components/TldrawEditor.tsx, components/workspace/CanvasPanel.tsx</files>
  <action>
**TldrawEditor.tsx changes:**
- Add useEditor hook from tldraw to access editor instance
- Expose editor ref via forwardRef or return editor from component
- OR: Create captureScreenshot() function that calls editor.exportAs('png', [shapeIds])
- Export screenshot capture capability

**CanvasPanel.tsx changes:**
- Import TldrawEditor with ref access
- Create state: canvasDataUrl (stores latest screenshot as base64 data URL)
- Add onChange handler that detects canvas changes
- Implement debounce logic: 2-3 second delay after last change
- On debounce trigger: call editor.exportAs('png') or equivalent
- Convert exported blob to base64 data URL: `URL.createObjectURL(blob)` or `FileReader.readAsDataURL(blob)`
- Store in canvasDataUrl state
- Trigger AI analysis callback (pass to ChatPanel via context or prop)

**Debounce implementation:**
Use setTimeout/clearTimeout pattern:
```typescript
const [debounceTimer, setDebounceTimer] = useState<NodeJS.Timeout | null>(null);

const handleCanvasChange = () => {
  if (debounceTimer) clearTimeout(debounceTimer);

  const timer = setTimeout(async () => {
    // Capture screenshot
    const blob = await editor.exportAs('png');
    const dataUrl = await blobToDataUrl(blob);
    setCanvasDataUrl(dataUrl);
    onCanvasCapture?.(dataUrl); // Notify ChatPanel
  }, 2500); // 2.5 seconds

  setDebounceTimer(timer);
};
```

**Canvas change detection:**
- tldraw provides `onChangePresence` or similar change events
- Alternative: useEffect watching editor.getCurrentPageShapeIds()
- Trigger on any shape add/update/delete

**Important:** Only capture full canvas, not just changed shapes. AI needs full context.
  </action>
  <verify>
    Draw on canvas → wait 3 seconds → console.log confirms screenshot captured
    Screenshot is valid base64 data URL
    Rapid drawing only captures once after pause (debounce works)
    TypeScript compilation passes
  </verify>
  <done>Canvas captures screenshots automatically on pause, stores as data URL, triggers callback</done>
</task>

<task type="auto">
  <name>Integrate canvas screenshots with AI chat API</name>
  <files>app/api/chat/route.ts</files>
  <action>
Update /api/chat route to accept and process canvas screenshots:

**Request body changes:**
- Add optional canvasImage field: `canvasImage?: string` (base64 data URL)

**AI SDK vision integration:**
Use multi-modal messages with image content:

```typescript
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai'; // GPT-4 Vision

export async function POST(req: Request) {
  const { messages, problem, canvasImage } = await req.json();

  const systemMessage = `${SYSTEM_PROMPT}

**Current Problem:**
${problem.text || 'Image uploaded'}

${canvasImage ? 'You can see the student\'s current canvas work. Analyze their progress and ask ONE guiding question.' : 'No canvas work yet. Ask an opening question.'}`;

  // Build messages with vision support
  const apiMessages = messages.map((msg: any) => ({
    role: msg.role,
    content: msg.content
  }));

  // Add canvas screenshot to last user message if present
  if (canvasImage && messages.length > 0) {
    const lastUserMsgIndex = apiMessages.map((m: any) => m.role).lastIndexOf('user');
    if (lastUserMsgIndex >= 0) {
      apiMessages[lastUserMsgIndex] = {
        role: 'user',
        content: [
          { type: 'text', text: messages[lastUserMsgIndex].content },
          { type: 'image', image: canvasImage }
        ]
      };
    }
  }

  const result = streamText({
    model: openai('gpt-4-turbo'), // or gpt-4o for better vision
    messages: [
      { role: 'system', content: systemMessage },
      ...apiMessages
    ],
    temperature: 0.7,
  });

  return result.toDataStreamResponse();
}
```

**Notes:**
- GPT-4 Turbo and GPT-4o support vision (image in messages)
- Claude 3.5 Sonnet also supports vision if using Anthropic
- Data URL format works directly (base64 encoded PNG)
- Image content should be in message content array alongside text

**Alternative if initial greeting:**
If messages=[] (initial greeting), attach canvas image as user message before AI responds.
  </action>
  <verify>
    POST /api/chat with canvasImage returns response referencing canvas content
    AI response mentions specific elements visible in screenshot
    Logs confirm image successfully sent to AI provider
  </verify>
  <done>AI chat API accepts canvas screenshots and sends to vision model for analysis</done>
</task>

<task type="auto">
  <name>Wire canvas capture to chat panel for analysis trigger</name>
  <files>components/workspace/ChatPanel.tsx, lib/problem-context.tsx</files>
  <action>
**Use Context-based approach:**
Extend ProblemProvider to include canvas state:
- Add canvasDataUrl state to problem-context.tsx
- Add setCanvasDataUrl() method
- CanvasPanel calls setCanvasDataUrl() when screenshot captured
- ChatPanel reads canvasDataUrl via useProblem()
- ChatPanel automatically sends analysis request when canvasDataUrl changes AND user hasn't typed recently

This approach is mandated because problem-context.tsx already exists and context is the established pattern for sharing workspace state.

**Implement auto-analysis flow:**
1. Canvas captures screenshot → updates canvasDataUrl state
2. ChatPanel detects canvasDataUrl change (useEffect)
3. Check: user hasn't sent message in last 5 seconds (avoid interrupting active typing)
4. If clear: automatically send analysis request to /api/chat
5. Analysis request: current messages + problem + canvasImage
6. Show "AI is analyzing your work..." in chat
7. AI response streams in

**Cancel in-flight analysis:**
- If user starts typing while analysis in progress: abort fetch request
- If canvas changes during analysis: cancel old request, queue new analysis after debounce

**State management:**
- isAnalyzing: boolean (show loading indicator)
- lastAnalysisTime: timestamp (prevent duplicate analysis)

Use AbortController for request cancellation:
```typescript
const abortControllerRef = useRef<AbortController | null>(null);

const analyzeCanvas = async () => {
  // Cancel previous request
  if (abortControllerRef.current) {
    abortControllerRef.current.abort();
  }

  abortControllerRef.current = new AbortController();

  fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({ messages, problem, canvasImage }),
    signal: abortControllerRef.current.signal
  });
};
```
  </action>
  <verify>
    Draw on canvas → pause 3 seconds → chat shows "AI is analyzing..."
    AI response references canvas content (e.g., "I see you've written X")
    Continue drawing → new analysis triggers after pause
    Type message while analysis in progress → analysis cancelled
    Build passes with no TypeScript errors
  </verify>
  <done>Canvas screenshots automatically trigger AI analysis, responses appear in chat with canvas awareness</done>
</task>

</tasks>

<verification>
**End-to-end flow:**
1. Enter workspace with problem
2. AI sends initial greeting
3. Start drawing on canvas
4. Pause for 3 seconds
5. Chat shows "AI is analyzing your work..."
6. AI responds with question about canvas content (e.g., "I see you've started with X. What's your next step?")
7. Continue drawing OR reply via text
8. Both triggers work: drawing pause → analysis, text send → response

**Technical checks:**
- Screenshot capture works (verify blob → data URL conversion)
- Debounce prevents excessive captures
- AI receives images (check network tab: payload includes image data)
- AI responses reference canvas (proves vision working)
- No memory leaks (old screenshots released)
- Build passes: `npm run build`
- TypeScript passes: `npx tsc --noEmit`

**Requirements coverage:**
- CANV-02: Canvas captures screenshot when user pauses ✓
- AI-01: AI receives canvas screenshot on user pause ✓
- AI-02: AI analyzes current work against original problem ✓
- AI-04: AI maintains conversation context (canvas + chat + problem) ✓
</verification>

<success_criteria>
- [ ] Canvas automatically captures screenshot after 2-3 second pause
- [ ] Screenshot sent to AI chat API as base64 data URL
- [ ] AI responses reference specific canvas content
- [ ] "Analyzing..." indicator shows during processing
- [ ] Rapid drawing only triggers one analysis (debounce works)
- [ ] User can draw OR chat, both trigger AI guidance
- [ ] Phase 2 complete: Full problem → canvas → AI analysis flow working
</success_criteria>

<output>
After completion, create `.planning/phases/02-problem-input---ai-integration/02-03-SUMMARY.md`
</output>
